version: "3.9"

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    network_mode: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:0.33
    container_name: openhands
    ports:
      - "3000:3000"
    depends_on:
      - ollama
    environment:
      SANDBOX_RUNTIME_CONTAINER_IMAGE: "docker.all-hands.dev/all-hands-ai/runtime:0.26-nikolaik"
      LOG_ALL_EVENTS: "true"
      LLM_API_KEY: "ollama"
      LLM_BASE_URL: "http://127.0.0.1:11434"
    volumes:
      - /run/user/10484/docker.sock:/var/run/docker.sock
      - ~/.openhands-state:/.openhands-state
    extra_hosts:
      - "host.docker.internal:host-gateway"
    stdin_open: true
    tty: true
    pull_policy: always
    restart: unless-stopped

volumes:
  ollama:
  openhands:
